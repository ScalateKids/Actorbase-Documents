**** Driver usage: Generic example
#+begin_src scala
  import com.actorbase.driver._
  import scala.pickling._, Defaults._
  import scala.pickling.binary._

  val client = ActorbaseDriver()
  val coll = client.addCollection("people")
  // just a "key" -> "value" pair
  coll.insert("key" -> "value")
  // creating a simple Person class
  case class Person(name: String, age: Int)
  // class Person must be serialized
  // e.g. with scala pickling
  val steven = Person("Steven", 64).pickle.value    // here we have an array of bytes
  val sylvester = Person("Steven", 70).pickle.value // serialized ready to be stored
  // insert Person type object..
  coll.insert("Seagal" -> steven)
  //..and another two
  coll.insert("Schwarzenegger" -> Person("Arnold", 68), "Stallone" -> sylvester)
  // finally two totally different item
  coll.insert("Foo" -> 42, "bar" -> "baz")
  // printing all collection contents
  for ((k, v) <- coll)
    println(s"$k -> $v")
  // find some keys inside collection and do operations
  for ((k, v) <- coll.find("Arnold", "Foo")) {
    // operations...
  }
  // using find and as to read contents
  val content = coll.find("Foo").as[Int]("Foo") // content = 42
  // get all collections owned on the database
  val myCollections = client.getCollections
  // drop collection 'people'
  myCollections.drop("people")
#+end_src

**** Modify user password
#+begin_src scala
scala> client.changePassword("newpw")
#+end_src

**** Creating a new collection
#+begin_src scala
scala> client.addCollection("consumers")
res0: com.actorbase.driver.data.ActorbaseCollection
#+end_src
**** Listing collections
#+begin_src scala
scala> client.listCollections
res0: List[String] = List(Map("anonymous" -> List("customers", 0)))
// owner -> (collections, size in bytes)
#+end_src
**** Inserting into a collection
#+begin_src scala
scala> client.getCollection("people").insert("keyOne" -> "valueOne", "keyTwo" -> 42)
res0: com.actorbase.driver.data.ActorbaseCollection

scala> client.getCollection("people").insert(ActorbaseObject("obj" -> "inserting with object"))
res1: com.actorbase.driver.data.ActorbaseCollection

scala> client.insert("people", false, "key" -> "value") // insert without update to collection "people"

scala> client.insertTo("people", false, "key" -> "value")("mike")
// insert without update to collection "people" owned by user mike
// just in case of being a contributor with write permissions, or admin
#+end_src

**** Find into a collection
#+begin_src scala
scala> client.find("keyOne", "people")
res0: com.actorbase.driver.data.ActorbaseObject

scala> client.findFrom("key", "people")("owner") // from admin, or a contributor
res1: com.actorbase.driver.data.ActorbaseObject

scala> coll.findOne("key") // from inside an ActorbaseCollection object
res2: com.actorbase.driver.data.ActorbaseObject
#+end_src

**** Remove item
#+begin_src scala
scala> client.getCollection("people").remove("people", "keyOne")
res0: com.actorbase.driver.data.ActorbaseCollection

scala> client.removeFrom("people", "customers")("owner") // remove people an customer owned by owner
res1: com.actorbase.driver.data.ActorbaseCollection
#+end_src

**** Printing a collection
#+begin_src scala
scala> println(client.getCollection("people"))
{
  "owner": "foo",
  "contributors": {},
  "collectionName": "people",
  "data": {
    "bar": "baz",
    "Foo": 42,
    "Seagal": {
        "type": "Person",
        "name": "Steven",
        "age": 64
    },
    "Schwarzenegger": {
        "type": "Person",
        "name": "Arnold",
        "age": 68
     },
    "Stallone": {
        "type": "Person",
        "name": "Sylvester",
        "age": 70
     }
  }
}

#+end_src

**** Printing some collections
#+begin_src scala
scala> client.getCollections.foreach(println)
#+end_src

**** Drop a collection
#+begin_src scala
scala> coll.drop
#+end_src

**** Drop some collections
#+begin_src scala
scala> client.dropCollections

scala> client.getCollections.drop

scala> client.dropCollections("customers", "people")

#+end_src

**** Count collections elements
#+begin_src scala
scala> println(auth.getCollection("people").count)
res0: Int = 3
#+end_src

**** Import from file
#+begin_src scala
scala> client.importData("foo/bar/baz.json")

#+end_src
**** Export to file
#+begin_src scala
scala> client.exportData("foo/bar/customers_people.json", "customers", "people")

scala> client.exportData("foo/bar/all_collections.json")

#+end_src
**** Contributor operations
#+begin_src scala
scala> val fooCollection = client.getCollection("foo")

scala> fooCollection.addContributor("aFriend", true) // add

scala> fooCollection.removeContributor("aFriend", true) // remove

scala> client.addContributor("aFriend", "people", false)
// aFriend is now contributor of collection "people" with read-only permissions

scala> client.addContributor("aFriend", "people", false, "mike")
// administrator operation: aFriend is now contributor of collection
// "people" with read-only permissions to the collection owned by mike
#+end_src

**** Administrative operations
#+begin_src scala
scala> client.addUser("aUser")

scala> client.removeUser("aUser")

scala> client.resetPassword("anotherUser")
#+end_src
**** build.sbt server
#+begin_src scala
name := "Actorbase-Server"
version := "1.0"
scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
  "com.typesafe.akka" %% "akka-actor" % "2.4.6",
  "com.typesafe.akka" %% "akka-testkit" % "2.4.6",
  "com.typesafe.akka" %% "akka-cluster-sharding" % "2.4.6",
  "com.typesafe.akka" %% "akka-cluster-tools" % "2.4.6",
  "com.typesafe.akka" % "akka-cluster-metrics_2.11" % "2.4.6",
  "com.typesafe.akka" %% "akka-slf4j" % "2.4.6",
  "com.typesafe" % "config" % "1.2.1",
  "org.scalatest" % "scalatest_2.11" % "2.2.6" % "test",
  "io.spray" %% "spray-can" % "1.3.3",
  "io.spray" %% "spray-routing" % "1.3.3",
  "io.spray" %% "spray-json" % "1.3.2",
  "org.mindrot" % "jbcrypt" % "0.3m",
  "com.github.t3hnar" % "scala-bcrypt_2.10" % "2.6",
  "com.github.romix.akka" %% "akka-kryo-serialization" % "0.4.1",
  "org.apache.maven.plugins" % "maven-shade-plugin" % "2.4.3",
  "ch.qos.logback" % "logback-classic" % "1.1.3"
)

javaOptions ++= Seq("-Xmx2048m")

#+end_src
**** build.sbt client
#+begin_src scala
name := "Actorbase-CLI"
version := "1.0"
scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
  "com.typesafe" % "config" % "1.2.1",
  "org.scala-lang.modules" %% "scala-parser-combinators" % "1.0.2",
  "org.scala-lang" % "jline" % "2.11.0-M3",
  "org.scalatest" % "scalatest_2.11" % "2.2.6" % "test",
  "org.scalaj" %% "scalaj-http" % "2.3.0",
  "com.netaporter" %% "pre-canned" % "0.0.8" % "test",
  "org.json4s" %% "json4s-native" % "3.3.0",
  "org.json4s" %% "json4s-jackson" % "3.3.0"
  )

#+end_src
**** build binaries
#+begin_src sh
$ sbt assembly
$ ./target/scala-2.11/actorbase.jar --config=path/to/config.cfg
#+end_src
**** build binaries2
#+begin_src sh
$ java -Dconfig.file=/dir/dir/reference.conf -jar actorbase.jar
#+end_src
**** configuration sample
#+begin_src scala

// storekeeper actors configuration
storekeepers {
  role = ""                // Optional: set role for storekeepers
  max-instances = 100000   // max instance inside the cluster
  instances-per-node = 20  // max instance per node, define storekeepers per collection
  size = 256               // number of keys stored per storekeeper
  save-method = "snapshot" // save method, can be 'snapshot' or 'onchange'
  snapshot-conf {             // snapshot configuration, mandatory for 'snapshot' save-method
    first-snapshot-after = 20 // ignored with 'onchange' policy
    snapshot-every = 50       // sets how many seconds between every save
  }
}

// storage configuration
persistence {
  save-folder = "actorbasedata/"      // folder where all data will be saved
  encryption-algorithm = "AES"        // encryption algorithm
  encryption-key = "hsujHu6UshHJslkV" // AES encryption key
}

name = actorbase  // actorsystem name
name = ${?NAME}   // Optional: env variable for actorsystem name

listen-on = "127.0.0.1"   // address listening for connections
listen-on = ${?LISTEN_ON} // Optional: env variable for listen address

exposed-port = 9999             // port open to connections
exposed-port = ${?EXPOSED_PORT} // Optional: env variable for port

seed-host = "127.0.0.1"   // base seed-node
seed-host = ${?SEED_HOST} // Optional: env variable for base seed-node

seed-port = 2500          // base seed-port
seed-port = ${?SEED_PORT} // Optional: env variable for base seed-port

shard-number = 40               //shard number for cluster sharding, rule of thumb: number of nodes x 10
shard-number = ${?SHARD_NUMBER} //Optional: env variable for shard number

// akka.cluster.use-dispatcher = cluster-dispatcher // cluster dispatcher, can be used to tune akka based
                                                    // on the system running the application
// cluster-dispatcher {
//  type = "Dispatcher"
//  executor = "fork-join-executor"
//  fork-join-executor {
//    parallelism-min = 2
//    parallelism-factor = 1.0
//    parallelism-max = 4
//  }
//   throughput = 100
//}

#+end_src
**** cluster configuration sample
#+begin_src scala
akka {

  // setting cluster actor ref
  actor{
    provider = "akka.cluster.ClusterActorRefProvider"

    // default mailbox type, using control aware dispatching and
    // unbound mailbox, beware of memory consumption

    default-mailbox.mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"

    // deployment of main actors
    deployment./main {

      // routing type
      // can be all akka provided routing strategy e.g. Round robin pool,
      // or consistent-hashing pool or even a custom one
      router = round-robin-pool
      cluster.allow-local-routees = on

      // max number of routees per nodes (e.g. main actor per node)
      cluster.max-nr-of-instances-per-node = 10
      seed-nodes = ["akka.tcp://actorbase@127.0.0.1:2500", "akka.tcp://actorbase@127.0.0.1:2501"]
      cluster.enabled = on
    }
  }

}
#+end_src
**** Basic actorbase conf
#+begin_src
akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "INFO"

  extensions = [
    "com.romix.akka.serialization.kryo.KryoSerializationExtension$",
    "akka.cluster.metrics.ClusterMetricsExtension",
    "akka.cluster.pubsub.DistributedPubSub"]

  actor {

    provider = "akka.cluster.ClusterActorRefProvider"

    default-mailbox {
      mailbox-type = "akka.dispatch.UnboundedControlAwareMailbox"
    }

  }

  cluster {
    auto-down-unreachable-after = off
    roles = [master]                               // set role for the node
    roles = ${?ROLES}                              // Optional: env variable for role
    min-nr-of-members = 1
    seed-nodes = [
      "akka.tcp://"${name}"@"${seed-host}":2500"]

    sharding {
      remember-entities = on
    }

    failure-detector {
      threshold = 12.0
      acceptable-heartbeat-pause = 25s
      heartbeat-interval = 5s
      heartbeat-request {
        expected-response-after = 20s
      }
    }
  }

  remote {
    log-remote-lifecycle-events = off
    maximum-payload-bytes = 100000000 bytes
    maximum-payload-bytes = ${?MAXIMUM_PAYLOAD_BYTES}

    netty.tcp {
      log-remote-lifecycle-events = off
      hostname = "127.0.0.1"
      port = ${seed-port}

      message-frame-size =  100000000b
      # message-frame-size = ${?MAXIMUM_PAYLOAD_BYTES}b

      send-buffer-size =  100000000b
      # send-buffer-size = ${?MAXIMUM_PAYLOAD_BYTES}b

      receive-buffer-size =  100000000b
      # receive-buffer-size =  ${?MAXIMUM_PAYLOAD_BYTES}b

      maximum-frame-size = 100000000b
      # maximum-frame-size =  ${?MAXIMUM_PAYLOAD_BYTES}b
    }

    transport-failure-detector {
      heartbeat-interval = 30 s
      acceptable-heartbeat-pause = 12 s
    }
  }
}

spray {
  io {
    read-buffer-size="4kspspray.io.tcp.keep-alive=1"
  }
  can {
    server {
      ssl-encryption = off  // SSL encryption
    }
  }
}

ssl {
  certificate-file = "cert/actorbase.com.jks" // certificate folder
  certificate-password = "vhjMYi9NRV"         // password for certification
}

akka.cluster.metrics.enabled=off
akka.persistence.journal.plugin = "akka.persistence.journal.inmem"
akka.persistence.snapshot-store.plugin = "akka.persistence.snapshot-store.local"
akka.log-dead-letters=off
#+end_src
**** JSON format
#+begin_src javascript
{
  owner: "Owner",
  contributors: {
    "mike": false, // readonly
    "john": true   // readwrite
  },
  collectionName: "foo",
  data: {
    "bar": "baz",
    "foobar": 42,
    "fooList": ["list", "of", "strings"]
  }
}
#+end_src
